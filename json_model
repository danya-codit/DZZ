import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import albumentations as A
from albumentations.pytorch import ToTensorV2
import numpy as np
from PIL import Image
import json
import matplotlib.pyplot as plt
from sklearn.utils.class_weight import compute_class_weight
from pycocotools import mask as maskUtils
import os
import segmentation_models_pytorch as smp
from google.colab import drive

# 1. –ú–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ Google Drive –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –ø—É—Ç–µ–π
drive.mount('/content/drive')
IMAGE_PATH = '/content/drive/MyDrive/–¶–∏—Ñ—Ä–æ–≤–∞—è –∫–∞—Ñ–µ–¥—Ä–∞/–°–∏–≥–º–µ–Ω—Ç–∞—Ü–∏—è/supervisely/full_area_01.png'
ANNOTATION_PATH = '/content/drive/MyDrive/–¶–∏—Ñ—Ä–æ–≤–∞—è –∫–∞—Ñ–µ–¥—Ä–∞/–°–∏–≥–º–µ–Ω—Ç–∞—Ü–∏—è/supervisely/instances.json'
SAVE_DIR = '/content/drive/MyDrive/–¶–∏—Ñ—Ä–æ–≤–∞—è –∫–∞—Ñ–µ–¥—Ä–∞/–°–∏–≥–º–µ–Ω—Ç–∞—Ü–∏—è/supervisely/models/'
os.makedirs(SAVE_DIR, exist_ok=True)

# 2. –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∫–ª–∞—Å—Å–æ–≤ –∏ —Ü–≤–µ—Ç–æ–≤
CLASSES = ['background', 'city', 'field', 'forest', 'water']  # –§–æ–Ω –≤—Å–µ–≥–¥–∞ –ø–µ—Ä–≤—ã–π (class 0)
CLASS_COLORS = {
    1: [255, 0, 0],    # –ö—Ä–∞—Å–Ω—ã–π - –≥–æ—Ä–æ–¥
    2: [255, 255, 0],  # –ñ–µ–ª—Ç—ã–π - –ø–æ–ª–µ
    3: [0, 255, 0],    # –ó–µ–ª–µ–Ω—ã–π - –ª–µ—Å
    4: [0, 0, 255]     # –°–∏–Ω–∏–π - –≤–æ–¥–∞
}
NUM_CLASSES = len(CLASSES)
DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'

# 3. –ö–ª–∞—Å—Å Dataset —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –≤—Å–µ—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ –º–∞—Å–æ–∫
class SatelliteDataset(Dataset):
    def __init__(self, image_path, annotation_path, transform=None):
        self.image = np.array(Image.open(image_path).convert('RGB'))
        with open(annotation_path) as f:
            self.coco_data = json.load(f)
        self.transform = transform
        self.masks = self._load_masks()
        
    def _load_masks(self):
        h, w = self.image.shape[:2]
        mask = np.zeros((h, w), dtype=np.uint8)
        
        for ann in self.coco_data['annotations']:
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π
            if isinstance(ann['segmentation'], list):
                # –ü–æ–ª–∏–≥–æ–Ω–∞–ª—å–Ω–∞—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è
                if len(ann['segmentation']) > 0:
                    rle = maskUtils.frPyObjects(ann['segmentation'], h, w)
                    m = maskUtils.decode(rle)
                    if m.ndim == 3:
                        m = np.sum(m, axis=2) > 0
            elif isinstance(ann['segmentation'], dict):
                # RLE –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è
                m = maskUtils.decode(ann['segmentation'])
            else:
                continue
                
            mask[m > 0] = ann['category_id']
            
        return mask
    
    def __len__(self):
        return 100  # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º 100 –∞—É–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤
    
    def __getitem__(self, idx):
        if self.transform:
            transformed = self.transform(image=self.image, mask=self.masks)
            return transformed['image'], transformed['mask']
        return self.image, self.masks

# 4. –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
def get_transform():
    return A.Compose([
        A.RandomCrop(512, 512),
        A.HorizontalFlip(p=0.5),
        A.VerticalFlip(p=0.5),
        A.RandomRotate90(p=0.5),
        A.RandomBrightnessContrast(p=0.2),
        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
        ToTensorV2()
    ])

# 5. –ú–æ–¥–µ–ª—å —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–≥–æ —ç–Ω–∫–æ–¥–µ—Ä–∞
class SegModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.model = smp.Unet(
            encoder_name="resnet34",
            encoder_weights="imagenet",
            classes=NUM_CLASSES,
            activation='softmax'
        )
        
    def forward(self, x):
        return self.model(x)

# 6. –§—É–Ω–∫—Ü–∏—è –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –≤–µ—Å–æ–≤ –∫–ª–∞—Å—Å–æ–≤
def get_class_weights(mask):
    present_classes = np.unique(mask)
    weights = compute_class_weight(
        'balanced',
        classes=present_classes,
        y=mask.flatten()
    )
    weight_dict = {cls: weight for cls, weight in zip(present_classes, weights)}
    return torch.tensor([weight_dict.get(i, 1.0) for i in range(NUM_CLASSES)]).float().to(DEVICE)

# 7. –§—É–Ω–∫—Ü–∏—è —Ä–∞—Å—á–µ—Ç–∞ IoU
def calculate_iou(preds, targets):
    smooth = 1e-6
    preds = torch.argmax(preds, dim=1)
    intersection = (preds == targets).float().sum()
    union = (preds != 0 | targets != 0).float().sum()
    return (intersection + smooth) / (union + smooth)

# 8. –û–±—É—á–µ–Ω–∏–µ —Å –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–µ–π
def train():
    dataset = SatelliteDataset(IMAGE_PATH, ANNOTATION_PATH, get_transform())
    loader = DataLoader(dataset, batch_size=4, shuffle=True)
    
    model = SegModel().to(DEVICE)
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)
    
    weights = get_class_weights(dataset.masks)
    criterion = nn.CrossEntropyLoss(weight=weights)
    
    train_loss = []
    train_iou = []
    best_iou = 0.0
    
    for epoch in range(20):
        model.train()
        epoch_loss = 0
        epoch_iou = 0
        
        for images, masks in loader:
            images, masks = images.to(DEVICE), masks.to(DEVICE)
            
            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, masks.long())
            loss.backward()
            optimizer.step()
            
            epoch_loss += loss.item()
            epoch_iou += calculate_iou(outputs, masks)
        
        epoch_loss /= len(loader)
        epoch_iou /= len(loader)
        train_loss.append(epoch_loss)
        train_iou.append(epoch_iou)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
        if epoch_iou > best_iou:
            best_iou = epoch_iou
            torch.save(model.state_dict(), os.path.join(SAVE_DIR, 'best_model.pth'))
            print(f'üéØ –ù–æ–≤—ã–π –ª—É—á—à–∏–π IoU: {best_iou:.4f}')
        
        print(f'Epoch {epoch+1}/{20}')
        print(f'Loss: {epoch_loss:.4f} | IoU: {epoch_iou:.4f}')
        print('-' * 30)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å
    torch.save(model.state_dict(), os.path.join(SAVE_DIR, 'final_model.pth'))
    
    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –º–µ—Ç—Ä–∏–∫
    plt.figure(figsize=(12, 5))
    plt.subplot(1, 2, 1)
    plt.plot(train_loss, 'r-', label='Loss')
    plt.title('Training Loss')
    plt.xlabel('Epoch')
    plt.grid(True)
    
    plt.subplot(1, 2, 2)
    plt.plot(train_iou, 'g-', label='IoU')
    plt.title('Training IoU')
    plt.xlabel('Epoch')
    plt.grid(True)
    
    plt.tight_layout()
    plt.savefig(os.path.join(SAVE_DIR, 'training_metrics.png'), dpi=300)
    plt.show()
    
    return model

# 9. –§—É–Ω–∫—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–µ–π
def predict_and_visualize(model, image_path):
    model.eval()
    image = np.array(Image.open(image_path).convert('RGB'))
    h, w = image.shape[:2]
    
    transform = A.Compose([
        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
        ToTensorV2()
    ])
    
    # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Ç–∞–π–ª—ã –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
    tile_size = 512
    full_mask = np.zeros((h, w), dtype=np.uint8)
    
    for y in range(0, h, tile_size):
        for x in range(0, w, tile_size):
            tile = image[y:y+tile_size, x:x+tile_size]
            if tile.size == 0:
                continue
                
            transformed = transform(image=tile)
            tile_tensor = transformed['image'].unsqueeze(0).to(DEVICE)
            
            with torch.no_grad():
                output = model(tile_tensor)
                pred = torch.argmax(output, dim=1).cpu().numpy()[0]
            
            full_mask[y:y+tile.shape[0], x:x+tile.shape[1]] = pred
    
    # –°–æ–∑–¥–∞–µ–º —Ü–≤–µ—Ç–Ω—É—é –º–∞—Å–∫—É
    color_mask = np.zeros((h, w, 3), dtype=np.uint8)
    for class_id, color in CLASS_COLORS.items():
        color_mask[full_mask == class_id] = color
    
    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
    plt.figure(figsize=(18, 6))
    
    plt.subplot(1, 3, 1)
    plt.imshow(image)
    plt.title('Original Image')
    plt.axis('off')
    
    plt.subplot(1, 3, 2)
    plt.imshow(color_mask)
    plt.title('Predicted Mask')
    plt.axis('off')
    
    plt.subplot(1, 3, 3)
    plt.imshow(image)
    plt.imshow(color_mask, alpha=0.5)
    plt.title('Overlay')
    plt.axis('off')
    
    # –õ–µ–≥–µ–Ω–¥–∞
    patches = [plt.Line2D([0], [0], marker='o', color='w', label=CLASSES[class_id],
               markerfacecolor=np.array(color)/255, markersize=10) 
              for class_id, color in CLASS_COLORS.items()]
    plt.legend(handles=patches, bbox_to_anchor=(1.05, 1), loc='upper left')
    
    plt.tight_layout()
    plt.show()

# 10. –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    print("–ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ...")
    trained_model = train()
    
    print("\n–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ –Ω–æ–≤–æ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏:")
    TEST_IMAGE = '/content/drive/MyDrive/–¶–∏—Ñ—Ä–æ–≤–∞—è –∫–∞—Ñ–µ–¥—Ä–∞/–°–∏–≥–º–µ–Ω—Ç–∞—Ü–∏—è/supervisely/full_area_02.png'
    predict_and_visualize(trained_model, TEST_IMAGE)
